{"meta":{"title":"啄木鸟的个人博客","subtitle":null,"description":null,"author":"WoodPecker","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"jieba中文处理的几个函数","slug":"结巴分词","date":"2019-07-19T02:13:00.000Z","updated":"2019-07-19T07:27:15.914Z","comments":true,"path":"2019/07/19/结巴分词/","link":"","permalink":"http://yoursite.com/2019/07/19/结巴分词/","excerpt":"","text":"一.基本分词函数jieba.cut() 接收三个输入参数： 需要分词的字符串 cut_all 参数来控制是否采用全模式 HMM参数用来控制是否使用 HMM 模型 12345seg_list = jieba.cut(&quot;我在学习自然语言处理&quot;, cut_all=True)print seg_listprint(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式seg_list = jieba.cut(&quot;我在学习自然语言处理&quot;, cut_all=False)print(&quot;Default Mode: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式 jieba.cut_for_search() 接收两个输入参数： 需要分词的字符串 是否使用 HMM 模型 12seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在哈佛大学深造&quot;) # 搜索引擎模式print(&quot;, &quot;.join(seg_list)) jieba.lcut() 123result_lcut = jieba.lcut(&quot;小明硕士毕业于中国科学院计算所，后在哈佛大学深造&quot;)print result_lcutprint &quot; &quot;.join(result_lcut) jieba.lcut_for_search() 1print &quot; &quot;.join(jieba.lcut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在哈佛大学深造&quot;)) jieba.load_userdict(file_name) 加载用户字典 suggest_freq(segment, tune=True) 调节单个词语的词频，使其能（不能）被分出来 1jieba.suggest_freq((&apos;中&apos;, &apos;将&apos;), True) jieba.tokenize() 返回词语在原文起止位置 123result = jieba.tokenize(u&apos;自然语言处理非常有用&apos;)for tk in result: print(&quot;%s\\t\\t start: %d \\t\\t end:%d&quot; % (tk[0],tk[1],tk[2])) 1234自然语言 start: 0 end:4处理 start: 4 end:6非常 start: 6 end:8有用 start: 8 end:10 二.关键词提取基于TD-IDF算法jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence 为待提取的文本 topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20 withWeight 为是否一并返回关键词权重值，默认值为 False allowPOS 仅包括指定词性的词，默认值为空，即不筛选 123import jieba.analyse as analyselines = open(&apos;文件&apos;).read()print (&quot; &quot;.join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=()))) jieba.analyse.set_idf_path(file_name) 关键词提取所使用的逆向文件频率（IDF）可自定义 jieba.analyse.set_stop_words(file_name) 关键词提取使用停止词可自定义 基于TextRankjieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’)) jieba.analyse.TextRank() 新建自定义 TextRank 实例 123import jieba.analyse as analyselines = open(&apos;NBA.txt&apos;).read()print(&quot; &quot;.join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=(&apos;ns&apos;, &apos;n&apos;, &apos;vn&apos;, &apos;v&apos;)))) 三.词性标注jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器 tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。 1234import jieba.posseg as psegwords = pseg.cut(&quot;我爱自然语言处理&quot;)for word, flag in words: print(&apos;%s %s&apos; % (word, flag))","categories":[],"tags":[]},{"title":"朴素贝叶斯","slug":"朴素贝叶斯","date":"2019-07-18T16:00:00.000Z","updated":"2019-07-19T07:50:41.403Z","comments":true,"path":"2019/07/19/朴素贝叶斯/","link":"","permalink":"http://yoursite.com/2019/07/19/朴素贝叶斯/","excerpt":"","text":"$$贝叶斯公式 + 条件独立假设 = 朴素贝叶斯方法$$ 一.贝叶斯公式$$P(Y|X)=P(X|Y)P(Y)／P(X)$$ $$由它推导：P(Y,X)=P(Y|X)P(X)=P(X|Y)P(Y)$$ 其中P(Y)P(Y)叫做先验概率，P(Y|X)P(Y|X)叫做后验概率，P(Y,X)P(Y,X)叫做联合概率。 在机器学习的视角下，我们把XX理解成“具有某特征”，把YY理解成“类别标签”(一般机器学习为题中都是X=&gt;特征, Y=&gt;结果对吧)。在最简单的二分类问题(是与否判定)下，我们将YY理解成“属于某类”的标签。于是贝叶斯公式就变形成了下面的样子:$$P(“属于某类”|“具有某特征”)=P(“具有某特征”|“属于某类”)P(“属于某类”)／P(“具有某特征”)$$ $$P(“属于某类”|“具有某特征”)= 在已知某样本“具有某特征”的条件下，该样本“属于某类”的概率。所以叫做『后验概率』。P(“具有某特征”|“属于某类”)=在已知某样本“属于某类”的条件下，该样本“具有某特征”的概率。P(“属于某类”)=（在未知某样本具有该“具有某特征”的条件下，）该样本“属于某类”的概率。所以叫做『先验概率』。P(“具有某特征”)=(在未知某样本“属于某类”的条件下，)该样本“具有某特征”的概率。$$","categories":[],"tags":[]}]}